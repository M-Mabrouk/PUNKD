{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependencies and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gdown\n",
    "import patoolib\n",
    "\n",
    "from preprocessing import get_data, get_transforms\n",
    "from monai.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from unet import FlexUNet, train_epoch, validate_epoch\n",
    "from metric import AllDiceMetric, SeparateDiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('archive.zip') and not os.path.exists('data'):\n",
    "    # Google Drive file ID\n",
    "    file_id = '1bz476ATbSduGcyw1UIkOkN5YkYZvurZ5'\n",
    "    # Destination path where the file will be saved\n",
    "    destination = 'archive.zip'\n",
    "\n",
    "    # Construct the download URL\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "    # Download the file\n",
    "    gdown.download(url, destination, quiet=False)\n",
    "    \n",
    "    patoolib.extract_archive('archive.zip', outdir='data')\n",
    "    os.remove('archive.zip')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files = get_data()\n",
    "transforms = get_transforms()\n",
    "\n",
    "print(f'Train files: {len(train_files)}')\n",
    "print(f'Test files: {len(val_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(data=train_files, transform=transforms)\n",
    "val_ds = Dataset(data=val_files, transform=transforms)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "WORKERS = 16\n",
    "\n",
    "model = FlexUNet(model_size=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "loss_fn = DiceLoss()\n",
    "\n",
    "all_dice_metric = AllDiceMetric(include_background=True)\n",
    "separate_dice_metric = SeparateDiceMetric()\n",
    "\n",
    "model_id = 'UNET_64_Teacher_01'\n",
    "file_id = model_id + '.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Dice Loss: {loss}')\n",
    "    total, tumor, enhancing, core = validate_epoch(model, val_loader, all_dice_metric, separate_dice_metric, device, all_dice_metric, separate_dice_metric)\n",
    "    print(f'Total Dice: {total}, Tumor Dice: {tumor}, Enhancing Dice: {enhancing}, Core Dice: {core}')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "torch.save(model.state_dict(), \"UNET_64_Teacher_01.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 credentials and endpoint\n",
    "access_key = os.environ.get('R2_KEY')\n",
    "secret_key = os.environ.get('R2_SECRET')\n",
    "endpoint_url = os.environ.get('R2_ENDPOINT')\n",
    "\n",
    "bucket_name = os.environ.get('R2_BUCKET')\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3',\n",
    "\t\t\t\t  endpoint_url=endpoint_url,\n",
    "\t\t\t\t  aws_access_key_id=access_key,\n",
    "\t\t\t\t  aws_secret_access_key=secret_key,\n",
    "\t\t\t\t  config=Config(signature_version='s3v4'))\n",
    "\n",
    "# Function to upload file with progress bar\n",
    "def upload_file_with_progress(file_path, bucket_name, object_name):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    progress = tqdm(total=file_size, unit='B', unit_scale=True, desc='Uploading ' + object_name)\n",
    "\n",
    "    def upload_progress(chunk):\n",
    "        progress.update(chunk)\n",
    "\n",
    "    s3.upload_file(file_path, bucket_name, object_name, Callback=upload_progress)\n",
    "    progress.close()\n",
    "\n",
    "# object details\n",
    "object_name = file_id\n",
    "file_path = file_id\n",
    "\n",
    "# Upload the file\n",
    "upload_file_with_progress(file_path, bucket_name, object_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
